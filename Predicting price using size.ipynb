{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.rea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import scipy as sp\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Convert the feature data to a DataFrame\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines in Scikitlearn\n",
    "* It is a way of chaining together multiple processing steps into a single object; enscapulating those steps into a single object\n",
    " \n",
    "    ##### Key Components of the Scikit-learn pipeline\n",
    "1. Transformer - they implement the fit and transform methods. Examples /*StandardScaler, MinMaxScaler*/  \n",
    "2. Estimator - They implement the fit and predict examples /*LinearRegression, RandomClassifier, SVC*/  \n",
    "3. Pipeline - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "#Loading the data \n",
    "iris_df = load_iris()\n",
    "\n",
    "iris_df\n",
    "\n",
    "X, y = iris_df.data, iris_df.target\n",
    "\n",
    "\n",
    "#Splitting the data into training and testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size= 0.2, random_state= 42)\n",
    "\n",
    "\n",
    "#Creating the pipeline\n",
    "\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()), # Step 1: Standardizing the features\n",
    "    ('pca', PCA(n_components= 2)), # Step 2: Reducing the dimensionality\n",
    "    ('clf', LogisticRegression()), # Step 3: The Classifer\n",
    "]\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "#Training the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Evaluating the pipeline\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy: .2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 2, 0, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 1,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scaler', StandardScaler()),\n",
       " ('pca', PCA(n_components=2)),\n",
       " ('clf', LogisticRegression())]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glob library\n",
    "- It is used to retrive files/pathname matching a specified pattern\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Usage :\n",
    "`The glob.glob()` is used to search for files/dir that match a specified pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Predicting price using size.ipynb', 'ImagesProcessing\\\\ImageProcessor.ipynb', 'Moringa\\\\eda.ipynb', 'Moringa\\\\Moringa DA Webinar  Class.ipynb', 'WorldQuant\\\\Housing in Argentina\\\\Predicting price using size.ipynb', 'WorldQuant\\\\Housing_Mexico\\\\Data Wrangling.ipynb', 'WorldQuant\\\\Housing_Mexico\\\\Exploratory Data Analysis.ipynb', 'WorldQuant\\\\Housing_Mexico\\\\Size or Location.ipynb', 'WorldQuant\\\\Housing_Mexico\\\\workstatioin.ipynb', 'WorldQuant\\\\Housing_Mexico\\\\Data\\\\mx.ipynb']\n"
     ]
    }
   ],
   "source": [
    "# List all .ipynb files in the current directory\n",
    "ipynb_files = glob.glob(\"**/*.ipynb\", recursive=True)\n",
    "print(ipynb_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wildcard patterns\n",
    "- `*` mathc any number of characters  \n",
    "- `?` match a single character of any kind  \n",
    "- `[a-z]`  lower case alphabet  \n",
    "- `[A-Z]` upper case  \n",
    "- `[!a-z]` not lower case  \n",
    "- `[0-9]` digits 0 to 9\n",
    "\n",
    "\n",
    "##### Recursive searching\n",
    "`**` is used to search for files in subdirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WorldQuant\\Housing_Mexico\\Data\\mexico-real-estate-1.csv\n",
      "WorldQuant\\Housing_Mexico\\Data\\mexico-real-estate-2.csv\n",
      "WorldQuant\\Housing_Mexico\\Data\\mexico-real-estate-3.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mexico_df_files = glob.glob(\"**/mexico*[0-9].csv\", recursive=True)\n",
    "\n",
    "[print(file) for file in mexico_df_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_Dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
